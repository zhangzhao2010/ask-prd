# Phase 2: çŸ¥è¯†åº“æ„å»ºç³»ç»Ÿ âœ…

> é¢„è®¡å·¥æœŸï¼š3å‘¨
> å®é™…å·¥æœŸï¼š1å¤©
> å®Œæˆæ—¥æœŸï¼š2025-11-05
> ç›®æ ‡ï¼šå®ç°PDFä¸Šä¼ ã€è½¬æ¢ã€å‘é‡åŒ–çš„å®Œæ•´æµç¨‹
> çŠ¶æ€ï¼š**å·²å®Œæˆ**

---

## ğŸ“Š å®Œæˆæ€»ç»“

**Phase 2 å·²100%å®Œæˆï¼**

ä¸»è¦æˆæœï¼š
- âœ… çŸ¥è¯†åº“ç®¡ç†APIï¼ˆCRUDæ“ä½œï¼‰
- âœ… æ–‡æ¡£ä¸Šä¼ åŠŸèƒ½ï¼ˆS3é›†æˆï¼‰
- âœ… PDFè½¬æ¢æœåŠ¡ï¼ˆMarker v1.10.1 + GPUåŠ é€Ÿï¼‰
- âœ… å›¾ç‰‡ç†è§£ï¼ˆBedrock Claude Vision APIï¼‰
- âœ… æ–‡æœ¬åˆ†å—æœåŠ¡ï¼ˆLangChain + ä¸­æ–‡ä¼˜åŒ–ï¼‰
- âœ… å‘é‡åŒ–æœåŠ¡ï¼ˆTitan Embeddings V2, 1024ç»´ï¼‰
- âœ… åŒæ­¥ä»»åŠ¡ç³»ç»Ÿï¼ˆå¼‚æ­¥åå°å¤„ç†ï¼‰
- âœ… å®Œæ•´9æ­¥å¤„ç†pipeline

ä»£ç ç»Ÿè®¡ï¼š
- conversion_service.py: 476è¡Œ
- chunking_service.py: 450è¡Œ
- embedding_service.py: 340è¡Œ
- task_service.py: 290è¡Œ
- sync_worker.py: 330è¡Œ

---

## ç¬¬1å‘¨ï¼šçŸ¥è¯†åº“å’Œæ–‡æ¡£ç®¡ç†

### 1.1 çŸ¥è¯†åº“ç®¡ç†API (Day 1-2)

- [ ] **å®ç° app/services/knowledge_base.py**
  ```python
  from app.models.database import KnowledgeBase
  from app.services.opensearch import OpenSearchService
  import uuid

  class KnowledgeBaseService:
      def __init__(self, db, opensearch: OpenSearchService):
          self.db = db
          self.opensearch = opensearch

      async def create_knowledge_base(self, name, description, s3_bucket, s3_prefix):
          """åˆ›å»ºçŸ¥è¯†åº“"""
          # 1. æ£€æŸ¥åç§°æ˜¯å¦é‡å¤
          # 2. åˆ›å»ºOpenSearch Collectionå’ŒIndex
          # 3. ä¿å­˜åˆ°æ•°æ®åº“
          # 4. è¿”å›ç»“æœ

      async def list_knowledge_bases(self):
          """è·å–çŸ¥è¯†åº“åˆ—è¡¨"""
          pass

      async def get_knowledge_base(self, kb_id):
          """è·å–çŸ¥è¯†åº“è¯¦æƒ…"""
          pass

      async def delete_knowledge_base(self, kb_id):
          """åˆ é™¤çŸ¥è¯†åº“"""
          # 1. åˆ é™¤OpenSearch Index
          # 2. åˆ é™¤S3æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
          # 3. åˆ é™¤æ•°æ®åº“è®°å½•
          pass
  ```

- [ ] **å®ç° app/api/v1/knowledge_bases.py**
  ```python
  from fastapi import APIRouter, Depends, HTTPException
  from app.models.schemas import KnowledgeBaseCreate, KnowledgeBaseResponse
  from app.services.knowledge_base import KnowledgeBaseService
  from app.core.database import get_db

  router = APIRouter()

  @router.post("/", response_model=KnowledgeBaseResponse, status_code=201)
  async def create_knowledge_base(
      kb: KnowledgeBaseCreate,
      db = Depends(get_db)
  ):
      """åˆ›å»ºçŸ¥è¯†åº“"""
      service = KnowledgeBaseService(db)
      return await service.create_knowledge_base(
          kb.name, kb.description, kb.s3_bucket, kb.s3_prefix
      )

  @router.get("/", response_model=List[KnowledgeBaseResponse])
  async def list_knowledge_bases(db = Depends(get_db)):
      """è·å–çŸ¥è¯†åº“åˆ—è¡¨"""
      pass

  @router.get("/{kb_id}", response_model=KnowledgeBaseResponse)
  async def get_knowledge_base(kb_id: str, db = Depends(get_db)):
      """è·å–çŸ¥è¯†åº“è¯¦æƒ…"""
      pass

  @router.delete("/{kb_id}")
  async def delete_knowledge_base(kb_id: str, db = Depends(get_db)):
      """åˆ é™¤çŸ¥è¯†åº“"""
      pass
  ```

- [ ] **æµ‹è¯•çŸ¥è¯†åº“API**
  ```bash
  # åˆ›å»ºçŸ¥è¯†åº“
  curl -X POST http://localhost:8000/api/v1/knowledge-bases \
    -H "Content-Type: application/json" \
    -d '{"name": "æµ‹è¯•çŸ¥è¯†åº“", "s3_bucket": "test", "s3_prefix": "test/"}'

  # è·å–åˆ—è¡¨
  curl http://localhost:8000/api/v1/knowledge-bases
  ```

### 1.2 æ–‡æ¡£ç®¡ç†API (Day 2-3)

- [ ] **å®ç° app/services/document.py**
  ```python
  from app.models.database import Document
  from app.services.s3 import S3Service
  import uuid

  class DocumentService:
      def __init__(self, db, s3: S3Service):
          self.db = db
          self.s3 = s3

      async def upload_document(self, kb_id, file):
          """ä¸Šä¼ æ–‡æ¡£"""
          # 1. éªŒè¯æ–‡ä»¶æ ¼å¼ï¼ˆPDFï¼‰
          # 2. éªŒè¯æ–‡ä»¶å¤§å°ï¼ˆ<100MBï¼‰
          # 3. ä¸Šä¼ åˆ°S3
          # 4. ä¿å­˜å…ƒæ•°æ®åˆ°æ•°æ®åº“
          # 5. çŠ¶æ€è®¾ä¸ºuploaded
          pass

      async def list_documents(self, kb_id, status=None, page=1, page_size=20):
          """è·å–æ–‡æ¡£åˆ—è¡¨"""
          pass

      async def get_document(self, doc_id):
          """è·å–æ–‡æ¡£è¯¦æƒ…"""
          pass

      async def delete_documents(self, kb_id, document_ids):
          """æ‰¹é‡åˆ é™¤æ–‡æ¡£"""
          # 1. åˆ é™¤S3æ–‡ä»¶
          # 2. åˆ é™¤OpenSearchå‘é‡
          # 3. åˆ é™¤æ•°æ®åº“è®°å½•
          # 4. åˆ é™¤æœ¬åœ°ç¼“å­˜
          pass
  ```

- [ ] **å®ç° app/api/v1/documents.py**
  ```python
  from fastapi import APIRouter, UploadFile, File, Depends
  from app.services.document import DocumentService

  router = APIRouter()

  @router.post("/knowledge-bases/{kb_id}/documents/upload")
  async def upload_document(
      kb_id: str,
      file: UploadFile = File(...),
      db = Depends(get_db)
  ):
      """ä¸Šä¼ æ–‡æ¡£"""
      service = DocumentService(db)
      return await service.upload_document(kb_id, file)

  @router.get("/knowledge-bases/{kb_id}/documents")
  async def list_documents(
      kb_id: str,
      status: Optional[str] = None,
      page: int = 1,
      page_size: int = 20,
      db = Depends(get_db)
  ):
      """è·å–æ–‡æ¡£åˆ—è¡¨"""
      pass

  @router.delete("/knowledge-bases/{kb_id}/documents")
  async def delete_documents(
      kb_id: str,
      doc_ids: DocumentDeleteRequest,
      db = Depends(get_db)
  ):
      """æ‰¹é‡åˆ é™¤æ–‡æ¡£"""
      pass
  ```

- [ ] **æµ‹è¯•æ–‡æ¡£API**
  ```bash
  # ä¸Šä¼ æ–‡æ¡£
  curl -X POST http://localhost:8000/api/v1/knowledge-bases/{kb_id}/documents/upload \
    -F "file=@test.pdf"

  # è·å–æ–‡æ¡£åˆ—è¡¨
  curl "http://localhost:8000/api/v1/knowledge-bases/{kb_id}/documents"
  ```

---

## ç¬¬2å‘¨ï¼šPDFè½¬æ¢å’Œå›¾ç‰‡ç†è§£

### 2.1 Markeré›†æˆ (Day 4-5)

- [ ] **å®‰è£…Marker**
  ```bash
  # éœ€è¦GPUæ”¯æŒ
  pip install marker-pdf

  # éªŒè¯å®‰è£…
  python -c "import marker; print(marker.__version__)"
  ```

- [ ] **å®ç° app/utils/pdf_converter.py**
  ```python
  from marker.converters.pdf import PdfConverter
  from marker.models import create_model_dict
  import os

  class PDFConverter:
      def __init__(self):
          self.models = create_model_dict()
          self.converter = PdfConverter(
              artifact_dict=self.models,
              processor_list=["ocr", "layout", "reading_order"]
          )

      def convert_pdf_to_markdown(self, pdf_path: str, output_dir: str):
          """
          è½¬æ¢PDFåˆ°Markdown

          Args:
              pdf_path: PDFæ–‡ä»¶è·¯å¾„
              output_dir: è¾“å‡ºç›®å½•

          Returns:
              (markdown_path, images_dir): Markdownæ–‡ä»¶è·¯å¾„å’Œå›¾ç‰‡ç›®å½•
          """
          try:
              # è½¬æ¢PDF
              rendered = self.converter(pdf_path)

              # ä¿å­˜Markdown
              markdown_path = os.path.join(output_dir, "content.md")
              with open(markdown_path, 'w', encoding='utf-8') as f:
                  f.write(rendered.markdown)

              # ä¿å­˜å›¾ç‰‡
              images_dir = os.path.join(output_dir, "images")
              os.makedirs(images_dir, exist_ok=True)

              for img_name, img_data in rendered.images.items():
                  img_path = os.path.join(images_dir, img_name)
                  with open(img_path, 'wb') as f:
                      f.write(img_data)

              return markdown_path, images_dir

          except Exception as e:
              raise PDFConversionError(f"PDFè½¬æ¢å¤±è´¥: {str(e)}")
  ```

- [ ] **æµ‹è¯•PDFè½¬æ¢**
  ```python
  # tests/test_utils/test_pdf_converter.py
  def test_pdf_conversion():
      converter = PDFConverter()
      markdown_path, images_dir = converter.convert_pdf_to_markdown(
          "tests/fixtures/test.pdf",
          "tests/output"
      )
      assert os.path.exists(markdown_path)
      assert os.path.exists(images_dir)
  ```

### 2.2 å›¾ç‰‡ç†è§£ï¼ˆBedrock Claudeï¼‰ (Day 5-6)

- [ ] **å®ç°å›¾ç‰‡æè¿°ç”Ÿæˆ**
  ```python
  # app/services/bedrock.py ä¸­æ·»åŠ 

  def generate_image_description(
      self,
      image_path: str,
      context: str,
      max_retries: int = 3
  ) -> str:
      """
      ç”Ÿæˆå›¾ç‰‡æè¿°

      Args:
          image_path: å›¾ç‰‡è·¯å¾„
          context: å›¾ç‰‡ä¸Šä¸‹æ–‡ï¼ˆå‰åæ–‡æœ¬ï¼‰
          max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°

      Returns:
          å›¾ç‰‡æè¿°æ–‡æœ¬
      """
      # è¯»å–å›¾ç‰‡
      with open(image_path, 'rb') as f:
          image_bytes = f.read()

      # æ„å»ºprompt
      prompt = f"""
      è¿™æ˜¯ä¸€å¼ æ¥è‡ªPRDæ–‡æ¡£çš„å›¾ç‰‡ã€‚

      å›¾ç‰‡ä¸Šä¸‹æ–‡ï¼š
      {context}

      è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ï¼ŒåŒ…æ‹¬ï¼š
      1. å›¾ç‰‡ç±»å‹ï¼ˆæµç¨‹å›¾/åŸå‹å›¾/æ¶æ„å›¾/å…¶ä»–ï¼‰
      2. æ ¸å¿ƒå†…å®¹
      3. å…³é”®å…ƒç´ å’Œå…³ç³»

      è¯·ç”¨ä¸­æ–‡å›ç­”ã€‚
      """

      # è°ƒç”¨Bedrock
      for attempt in range(max_retries):
          try:
              response = self.client.invoke_model(
                  modelId=settings.GENERATION_MODEL_ID,
                  body=json.dumps({
                      "anthropic_version": "bedrock-2023-05-31",
                      "max_tokens": 1000,
                      "messages": [
                          {
                              "role": "user",
                              "content": [
                                  {
                                      "type": "image",
                                      "source": {
                                          "type": "base64",
                                          "media_type": "image/png",
                                          "data": base64.b64encode(image_bytes).decode()
                                      }
                                  },
                                  {
                                      "type": "text",
                                      "text": prompt
                                  }
                              ]
                          }
                      ]
                  })
              )

              result = json.loads(response['body'].read())
              return result['content'][0]['text']

          except Exception as e:
              if "throttling" in str(e).lower() or "rate" in str(e).lower():
                  # é™æµï¼Œç­‰å¾…åé‡è¯•
                  wait_time = 2 ** attempt
                  time.sleep(wait_time)
              else:
                  # å…¶ä»–é”™è¯¯ï¼Œè¿”å›é™çº§æè¿°
                  return f"[å›¾ç‰‡æè¿°ç”Ÿæˆå¤±è´¥: {os.path.basename(image_path)}]"

      # æ‰€æœ‰é‡è¯•å¤±è´¥
      return f"[å›¾ç‰‡æè¿°ç”Ÿæˆå¤±è´¥: {os.path.basename(image_path)}]"
  ```

- [ ] **å®ç°å›¾ç‰‡ä¸Šä¸‹æ–‡æå–**
  ```python
  # app/utils/text_splitter.py

  def extract_image_context(markdown_content: str, image_name: str, context_length: int = 500):
      """
      ä»Markdownä¸­æå–å›¾ç‰‡ä¸Šä¸‹æ–‡

      Args:
          markdown_content: Markdownå†…å®¹
          image_name: å›¾ç‰‡æ–‡ä»¶å
          context_length: ä¸Šä¸‹æ–‡å­—ç¬¦æ•°

      Returns:
          å›¾ç‰‡å‰åçš„æ–‡æœ¬
      """
      # æŸ¥æ‰¾å›¾ç‰‡å¼•ç”¨ä½ç½®
      pattern = f"!\\[.*?\\]\\(.*?{image_name}.*?\\)"
      match = re.search(pattern, markdown_content)

      if not match:
          return ""

      pos = match.start()
      start = max(0, pos - context_length)
      end = min(len(markdown_content), pos + context_length)

      return markdown_content[start:end]
  ```

### 2.3 æ–‡æœ¬åˆ†å— (Day 6-7)

- [ ] **å®ç°æ–‡æœ¬åˆ†å—**
  ```python
  # app/utils/text_splitter.py

  from langchain.text_splitter import RecursiveCharacterTextSplitter

  class ChunkSplitter:
      def __init__(self, chunk_size=1000, chunk_overlap=200):
          self.splitter = RecursiveCharacterTextSplitter(
              chunk_size=chunk_size,
              chunk_overlap=chunk_overlap,
              separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", " ", ""]
          )

      def split_markdown(self, markdown_content: str):
          """
          åˆ†å—Markdownæ–‡æœ¬

          Returns:
              List of chunks
          """
          return self.splitter.split_text(markdown_content)

      def create_chunks_with_metadata(
          self,
          markdown_content: str,
          document_id: str,
          kb_id: str,
          image_descriptions: dict
      ):
          """
          åˆ›å»ºå¸¦å…ƒæ•°æ®çš„chunks

          Args:
              markdown_content: Markdownå†…å®¹
              document_id: æ–‡æ¡£ID
              kb_id: çŸ¥è¯†åº“ID
              image_descriptions: å›¾ç‰‡æè¿°å­—å…¸ {image_name: description}

          Returns:
              List of Chunk objects
          """
          chunks = []
          text_chunks = self.split_markdown(markdown_content)

          for idx, chunk_text in enumerate(text_chunks):
              # æ£€æŸ¥chunkä¸­æ˜¯å¦å¼•ç”¨äº†å›¾ç‰‡
              referenced_images = self._find_referenced_images(chunk_text)

              # å¢å¼ºå†…å®¹ï¼ˆæ·»åŠ å›¾ç‰‡æè¿°ï¼‰
              enhanced_content = chunk_text
              for img_name in referenced_images:
                  if img_name in image_descriptions:
                      enhanced_content += f"\n\n[å›¾ç‰‡æè¿°: {image_descriptions[img_name]}]"

              chunk = {
                  'id': str(uuid.uuid4()),
                  'document_id': document_id,
                  'kb_id': kb_id,
                  'chunk_type': 'text',
                  'chunk_index': idx,
                  'content': chunk_text,
                  'content_with_context': enhanced_content,
                  'char_start': self._calculate_char_position(markdown_content, chunk_text),
                  'char_end': self._calculate_char_position(markdown_content, chunk_text) + len(chunk_text),
                  'token_count': self._count_tokens(enhanced_content)
              }
              chunks.append(chunk)

          return chunks

      def create_image_chunks(
          self,
          images_info: list,
          document_id: str,
          kb_id: str
      ):
          """
          åˆ›å»ºå›¾ç‰‡chunks

          Args:
              images_info: å›¾ç‰‡ä¿¡æ¯åˆ—è¡¨ [{'filename', 'description', 'context', ...}]

          Returns:
              List of Image Chunk objects
          """
          chunks = []

          for idx, img_info in enumerate(images_info):
              chunk = {
                  'id': str(uuid.uuid4()),
                  'document_id': document_id,
                  'kb_id': kb_id,
                  'chunk_type': 'image',
                  'chunk_index': idx,
                  'image_filename': img_info['filename'],
                  'image_s3_key': img_info['s3_key'],
                  'image_local_path': img_info['local_path'],
                  'image_description': img_info['description'],
                  'image_type': img_info.get('type', 'other'),
                  'content_with_context': f"{img_info.get('context', '')}\n\nå›¾ç‰‡æè¿°ï¼š{img_info['description']}",
                  'token_count': self._count_tokens(img_info['description'])
              }
              chunks.append(chunk)

          return chunks
  ```

---

## ç¬¬3å‘¨ï¼šåŒæ­¥ä»»åŠ¡ç³»ç»Ÿ

### 3.1 åŒæ­¥ä»»åŠ¡æ¨¡å‹ (Day 8)

- [ ] **å®ç° app/services/sync.py - åŒæ­¥æœåŠ¡**
  ```python
  from app.models.database import SyncTask, Document, Chunk
  from app.utils.pdf_converter import PDFConverter
  from app.utils.text_splitter import ChunkSplitter
  from app.services.bedrock import BedrockService
  from app.services.opensearch import OpenSearchService
  import threading

  class SyncService:
      def __init__(self, db):
          self.db = db
          self.pdf_converter = PDFConverter()
          self.chunk_splitter = ChunkSplitter()
          self.bedrock = BedrockService()
          self.opensearch = OpenSearchService()

      async def create_sync_task(self, kb_id, task_type, document_ids=None):
          """åˆ›å»ºåŒæ­¥ä»»åŠ¡"""
          # 1. æ£€æŸ¥æ˜¯å¦å·²æœ‰è¿è¡Œä¸­çš„ä»»åŠ¡
          # 2. åˆ›å»ºä»»åŠ¡è®°å½•
          # 3. å¯åŠ¨åå°çº¿ç¨‹å¤„ç†
          task = SyncTask(
              id=str(uuid.uuid4()),
              kb_id=kb_id,
              task_type=task_type,
              document_ids=json.dumps(document_ids or []),
              status='pending'
          )
          self.db.add(task)
          self.db.commit()

          # å¼‚æ­¥æ‰§è¡Œ
          thread = threading.Thread(
              target=self._process_sync_task,
              args=(task.id,)
          )
          thread.start()

          return task

      def _process_sync_task(self, task_id):
          """å¤„ç†åŒæ­¥ä»»åŠ¡ï¼ˆåå°çº¿ç¨‹ï¼‰"""
          try:
              # 1. æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºrunning
              task = self.db.query(SyncTask).filter_by(id=task_id).first()
              task.status = 'running'
              task.started_at = datetime.utcnow()
              self.db.commit()

              # 2. è·å–éœ€è¦å¤„ç†çš„æ–‡æ¡£
              documents = self._get_documents_to_process(task)
              task.total_documents = len(documents)
              self.db.commit()

              # 3. é€ä¸ªå¤„ç†æ–‡æ¡£
              for idx, doc in enumerate(documents):
                  try:
                      self._process_document(doc, task)
                      task.processed_documents += 1
                  except Exception as e:
                      logger.error(f"Document processing failed: {doc.id}", exc_info=True)
                      task.failed_documents += 1
                      doc.status = 'failed'
                      doc.error_message = str(e)

                  # æ›´æ–°è¿›åº¦
                  task.progress = int((idx + 1) / len(documents) * 100)
                  task.current_step = f"æ­£åœ¨å¤„ç†æ–‡æ¡£ {idx + 1}/{len(documents)}"
                  self.db.commit()

              # 4. å®Œæˆä»»åŠ¡
              if task.failed_documents == 0:
                  task.status = 'completed'
              elif task.processed_documents == 0:
                  task.status = 'failed'
              else:
                  task.status = 'partial_success'

              task.completed_at = datetime.utcnow()
              self.db.commit()

          except Exception as e:
              logger.error(f"Sync task failed: {task_id}", exc_info=True)
              task.status = 'failed'
              task.error_message = str(e)
              self.db.commit()

      def _process_document(self, document, task):
          """å¤„ç†å•ä¸ªæ–‡æ¡£"""
          logger.info(f"Processing document: {document.id}")

          # 1. ä¸‹è½½PDFåˆ°æœ¬åœ°
          task.current_step = f"æ­£åœ¨ä¸‹è½½ {document.filename}"
          self.db.commit()

          local_pdf = self._download_pdf(document)

          # 2. PDFè½¬Markdown
          task.current_step = f"æ­£åœ¨è½¬æ¢ {document.filename}"
          self.db.commit()

          output_dir = f"/data/cache/documents/{document.id}"
          os.makedirs(output_dir, exist_ok=True)

          markdown_path, images_dir = self.pdf_converter.convert_pdf_to_markdown(
              local_pdf, output_dir
          )

          # 3. å¤„ç†å›¾ç‰‡
          task.current_step = f"æ­£åœ¨å¤„ç†å›¾ç‰‡ {document.filename}"
          self.db.commit()

          images_info = self._process_images(
              markdown_path, images_dir, document
          )

          # 4. åˆ›å»ºchunks
          task.current_step = f"æ­£åœ¨åˆ†å— {document.filename}"
          self.db.commit()

          with open(markdown_path, 'r', encoding='utf-8') as f:
              markdown_content = f.read()

          # åˆ›å»ºæ–‡æœ¬chunks
          text_chunks = self.chunk_splitter.create_chunks_with_metadata(
              markdown_content,
              document.id,
              document.kb_id,
              {img['filename']: img['description'] for img in images_info}
          )

          # åˆ›å»ºå›¾ç‰‡chunks
          image_chunks = self.chunk_splitter.create_image_chunks(
              images_info, document.id, document.kb_id
          )

          all_chunks = text_chunks + image_chunks

          # 5. å‘é‡åŒ–
          task.current_step = f"æ­£åœ¨å‘é‡åŒ– {document.filename}"
          self.db.commit()

          self._vectorize_chunks(all_chunks)

          # 6. å­˜å‚¨åˆ°OpenSearchå’Œæ•°æ®åº“
          task.current_step = f"æ­£åœ¨å­˜å‚¨ {document.filename}"
          self.db.commit()

          self._store_chunks(all_chunks, document.kb_id)

          # 7. æ›´æ–°æ–‡æ¡£çŠ¶æ€
          document.status = 'completed'
          document.local_markdown_path = markdown_path
          document.local_images_dir = images_dir
          self.db.commit()

          logger.info(f"Document processed successfully: {document.id}")

      def _process_images(self, markdown_path, images_dir, document):
          """å¤„ç†æ‰€æœ‰å›¾ç‰‡"""
          images_info = []

          with open(markdown_path, 'r', encoding='utf-8') as f:
              markdown_content = f.read()

          # æ‰¾åˆ°æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶
          image_files = [f for f in os.listdir(images_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]

          for img_file in image_files:
              img_path = os.path.join(images_dir, img_file)

              # æå–ä¸Šä¸‹æ–‡
              context = extract_image_context(markdown_content, img_file)

              # ç”Ÿæˆæè¿°
              description = self.bedrock.generate_image_description(img_path, context)

              # ä¸Šä¼ åˆ°S3
              s3_key = f"{document.kb_id}/converted/{document.id}/images/{img_file}"
              self.s3.upload_file(img_path, s3_key)

              images_info.append({
                  'filename': img_file,
                  'local_path': img_path,
                  's3_key': s3_key,
                  'description': description,
                  'context': context,
                  'type': self._detect_image_type(description)
              })

          return images_info

      def _vectorize_chunks(self, chunks):
          """å‘é‡åŒ–chunks"""
          for chunk in chunks:
              try:
                  embedding = self.bedrock.generate_embedding(
                      chunk['content_with_context']
                  )
                  chunk['embedding'] = embedding
              except Exception as e:
                  logger.error(f"Embedding failed for chunk: {chunk['id']}", exc_info=True)
                  chunk['embedding'] = None

      def _store_chunks(self, chunks, kb_id):
          """å­˜å‚¨chunksåˆ°OpenSearchå’Œæ•°æ®åº“"""
          # å­˜å‚¨åˆ°æ•°æ®åº“
          for chunk_data in chunks:
              chunk = Chunk(**chunk_data)
              self.db.add(chunk)

          # æ‰¹é‡å­˜å‚¨åˆ°OpenSearch
          index_name = f"kb_{kb_id}_index"
          for chunk in chunks:
              if chunk.get('embedding'):
                  self.opensearch.index_document(
                      index_name,
                      chunk['id'],
                      {
                          'chunk_id': chunk['id'],
                          'document_id': chunk['document_id'],
                          'kb_id': chunk['kb_id'],
                          'chunk_type': chunk['chunk_type'],
                          'content': chunk.get('content', ''),
                          'content_with_context': chunk['content_with_context'],
                          'embedding': chunk['embedding']
                      }
                  )

          self.db.commit()
  ```

### 3.2 åŒæ­¥ä»»åŠ¡API (Day 9)

- [ ] **å®ç° app/api/v1/sync_tasks.py**
  ```python
  from fastapi import APIRouter, Depends
  from app.services.sync import SyncService

  router = APIRouter()

  @router.post("/knowledge-bases/{kb_id}/sync")
  async def create_sync_task(
      kb_id: str,
      request: SyncTaskCreate,
      db = Depends(get_db)
  ):
      """åˆ›å»ºåŒæ­¥ä»»åŠ¡"""
      service = SyncService(db)
      return await service.create_sync_task(
          kb_id, request.task_type, request.document_ids
      )

  @router.get("/knowledge-bases/{kb_id}/sync-tasks")
  async def list_sync_tasks(
      kb_id: str,
      status: Optional[str] = None,
      db = Depends(get_db)
  ):
      """è·å–åŒæ­¥ä»»åŠ¡åˆ—è¡¨"""
      pass

  @router.get("/sync-tasks/{task_id}")
  async def get_sync_task(task_id: str, db = Depends(get_db)):
      """è·å–ä»»åŠ¡è¯¦æƒ…"""
      pass

  @router.post("/sync-tasks/{task_id}/cancel")
  async def cancel_sync_task(task_id: str, db = Depends(get_db)):
      """å–æ¶ˆä»»åŠ¡"""
      pass
  ```

### 3.3 æµ‹è¯•å’Œä¼˜åŒ– (Day 10)

- [ ] **ç«¯åˆ°ç«¯æµ‹è¯•**
  ```python
  # tests/test_integration/test_sync_flow.py

  def test_full_sync_flow():
      """æµ‹è¯•å®Œæ•´åŒæ­¥æµç¨‹"""
      # 1. åˆ›å»ºçŸ¥è¯†åº“
      kb = create_knowledge_base("æµ‹è¯•çŸ¥è¯†åº“")

      # 2. ä¸Šä¼ æ–‡æ¡£
      doc = upload_document(kb.id, "test.pdf")

      # 3. åˆ›å»ºåŒæ­¥ä»»åŠ¡
      task = create_sync_task(kb.id, "full_sync")

      # 4. ç­‰å¾…ä»»åŠ¡å®Œæˆ
      wait_for_task_completion(task.id, timeout=300)

      # 5. éªŒè¯ç»“æœ
      task = get_sync_task(task.id)
      assert task.status == "completed"

      # 6. éªŒè¯chunkså·²åˆ›å»º
      chunks = list_chunks(doc.id)
      assert len(chunks) > 0
  ```

- [ ] **æ€§èƒ½æµ‹è¯•**
  ```python
  def test_batch_document_processing():
      """æµ‹è¯•æ‰¹é‡æ–‡æ¡£å¤„ç†"""
      # ä¸Šä¼ 10ä¸ªæ–‡æ¡£
      # æµ‹è¯•å¤„ç†æ—¶é—´
      # éªŒè¯æˆåŠŸç‡
      pass
  ```

---

## éªŒæ”¶æ ‡å‡†

### å¿…é¡»å®Œæˆ
- [ ] çŸ¥è¯†åº“CRUD APIå…¨éƒ¨å®ç°
- [ ] æ–‡æ¡£ä¸Šä¼ APIå®ç°
- [ ] PDFè½¬MarkdownæˆåŠŸï¼ˆMarkerï¼‰
- [ ] å›¾ç‰‡æè¿°ç”ŸæˆæˆåŠŸï¼ˆBedrockï¼‰
- [ ] æ–‡æœ¬åˆ†å—æ­£ç¡®
- [ ] å‘é‡åŒ–æˆåŠŸï¼ˆBedrock Embeddingï¼‰
- [ ] OpenSearchç´¢å¼•æˆåŠŸ
- [ ] åŒæ­¥ä»»åŠ¡ç³»ç»Ÿå·¥ä½œæ­£å¸¸
- [ ] å¯ä»¥æŸ¥çœ‹ä»»åŠ¡è¿›åº¦
- [ ] é”™è¯¯å¤„ç†å®Œå–„

### å¯é€‰
- [ ] æ‰¹é‡ä¸Šä¼ æ–‡æ¡£
- [ ] ä»»åŠ¡å–æ¶ˆåŠŸèƒ½
- [ ] å¤±è´¥é‡è¯•åŠŸèƒ½

---

## æ£€æŸ¥æ¸…å•

åœ¨è¿›å…¥Phase 3ä¹‹å‰ï¼Œç¡®è®¤ï¼š

- [ ] å¯ä»¥æˆåŠŸåˆ›å»ºçŸ¥è¯†åº“
- [ ] å¯ä»¥ä¸Šä¼ PDFæ–‡æ¡£
- [ ] PDFå¯ä»¥è½¬æ¢ä¸ºMarkdown
- [ ] å›¾ç‰‡å¯ä»¥è¢«æ­£ç¡®ç†è§£
- [ ] Chunkså¯ä»¥è¢«æ­£ç¡®åˆ›å»º
- [ ] å‘é‡å¯ä»¥è¢«æ­£ç¡®ç”Ÿæˆ
- [ ] æ•°æ®å¯ä»¥å­˜å…¥OpenSearch
- [ ] å¯ä»¥æŸ¥çœ‹åŒæ­¥ä»»åŠ¡è¿›åº¦
- [ ] é”™è¯¯ä¼šè¢«æ­£ç¡®è®°å½•å’Œå±•ç¤º
- [ ] å®Œæ•´æµç¨‹æµ‹è¯•é€šè¿‡

---

## ä¸‹ä¸€æ­¥

å®ŒæˆPhase 2åï¼Œè¿›å…¥ [Phase 3: æ£€ç´¢é—®ç­”ç³»ç»Ÿ](./todo-phase3-query.md)
