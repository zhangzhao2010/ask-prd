# Phase 3: æ£€ç´¢é—®ç­”ç³»ç»Ÿ âœ…

> é¢„è®¡å·¥æœŸï¼š3å‘¨
> å®é™…å·¥æœŸï¼š1å¤©
> å®Œæˆæ—¥æœŸï¼š2025-11-05
> ç›®æ ‡ï¼šå®ç°åŸºäºMulti-Agentçš„æ™ºèƒ½é—®ç­”
> çŠ¶æ€ï¼š**å·²å®Œæˆ**

---

## ğŸ“Š å®Œæˆæ€»ç»“

**Phase 3 å·²100%å®Œæˆï¼**

ä¸»è¦æˆæœï¼š
- âœ… Query Rewriteï¼ˆæŸ¥è¯¢ä¼˜åŒ–ï¼‰
- âœ… Hybrid Searchï¼ˆå‘é‡ + BM25 + RRFï¼‰
- âœ… Sub-Agentç³»ç»Ÿï¼ˆStrandsæ¡†æ¶å®ç°ï¼‰
- âœ… Main-Agentç»¼åˆï¼ˆæµå¼è¾“å‡ºï¼‰
- âœ… SSEæµå¼è¾“å‡ºï¼ˆç¬¦åˆdocsè§„èŒƒï¼‰
- âœ… Citationå¼•ç”¨æå–ï¼ˆæ–‡æœ¬+å›¾ç‰‡ï¼‰
- âœ… Tokenç»Ÿè®¡å’ŒæŸ¥è¯¢å†å²
- âœ… å›¾ç‰‡URLæ¥å£
- âœ… æŸ¥è¯¢å†å²ç®¡ç†

ä»£ç ç»Ÿè®¡ï¼š
- query_service.py: 480è¡Œ
- query/routes.py: 217è¡Œ
- sub_agent.py: 230è¡Œ
- main_agent.py: 220è¡Œ
- document_tools.py: 100è¡Œ
- chunks/routes.py: 80è¡Œ

æŠ€æœ¯äº®ç‚¹ï¼š
- ğŸ¤– Multi-Agentæ¶æ„ï¼ˆStrands SDKï¼‰
- ğŸ“Š Hybrid Searchï¼ˆRRFç®—æ³•ï¼‰
- âš¡ SSEå®æ—¶æµå¼è¾“å‡º
- ğŸ”§ å¹¶å‘æ§åˆ¶ï¼ˆSemaphoreé™æµï¼‰
- ğŸ“ˆ å®Œæ•´çš„Metricsæ”¶é›†

---

## ç¬¬1å‘¨ï¼šæ£€ç´¢ç³»ç»Ÿ

### 1.1 Query Rewrite (Day 1-2)

- [ ] **å®ç°æŸ¥è¯¢é‡å†™**
  ```python
  # app/services/query.py

  class QueryService:
      def __init__(self, bedrock: BedrockService):
          self.bedrock = bedrock

      async def rewrite_query(self, original_query: str) -> List[str]:
          """
          é‡å†™ç”¨æˆ·æŸ¥è¯¢ï¼Œç”Ÿæˆå¤šä¸ªæ£€ç´¢å˜ä½“

          Args:
              original_query: ç”¨æˆ·åŸå§‹æŸ¥è¯¢

          Returns:
              List of rewritten queries
          """
          prompt = f"""
          ç”¨æˆ·çš„åŸå§‹é—®é¢˜æ˜¯ï¼š"{original_query}"

          è¯·å°†è¿™ä¸ªé—®é¢˜æ”¹å†™æˆ3ä¸ªä¸åŒçš„æ£€ç´¢æŸ¥è¯¢ï¼Œä»¥æé«˜å¬å›ç‡ã€‚
          æ”¹å†™è¦æ±‚ï¼š
          1. ä¿æŒåŸæ„
          2. ä½¿ç”¨ä¸åŒçš„è¡¨è¾¾æ–¹å¼
          3. æ·»åŠ å¯èƒ½çš„åŒä¹‰è¯æˆ–ç›¸å…³æœ¯è¯­
          4. é€‚åˆå‘é‡æ£€ç´¢å’Œå…³é”®è¯æ£€ç´¢

          è¯·ç›´æ¥è¾“å‡º3ä¸ªæ”¹å†™åçš„æŸ¥è¯¢ï¼Œæ¯è¡Œä¸€ä¸ªï¼Œä¸è¦ç¼–å·ã€‚
          """

          response = self.bedrock.generate_text(prompt)

          # è§£æå“åº”
          rewritten_queries = [q.strip() for q in response.strip().split('\n') if q.strip()]

          # åŒ…å«åŸå§‹æŸ¥è¯¢
          return [original_query] + rewritten_queries[:3]
  ```

- [ ] **æµ‹è¯•Query Rewrite**
  ```python
  # tests/test_services/test_query.py

  def test_query_rewrite():
      service = QueryService(bedrock)
      queries = await service.rewrite_query("ç™»å½•æ³¨å†Œæ¨¡å—çš„æ¼”è¿›å†å²")

      assert len(queries) >= 2  # è‡³å°‘åŸå§‹+1ä¸ªæ”¹å†™
      assert "ç™»å½•" in queries[0] or "æ³¨å†Œ" in queries[0]
  ```

### 1.2 Hybrid Search (Day 2-4)

- [ ] **å®ç°å‘é‡æ£€ç´¢**
  ```python
  # app/services/opensearch.py ä¸­æ·»åŠ 

  def vector_search(
      self,
      index_name: str,
      query_vector: List[float],
      top_k: int = 20,
      filters: dict = None
  ):
      """
      å‘é‡æ£€ç´¢

      Args:
          index_name: ç´¢å¼•åç§°
          query_vector: æŸ¥è¯¢å‘é‡
          top_k: è¿”å›æ•°é‡
          filters: è¿‡æ»¤æ¡ä»¶

      Returns:
          List of search results
      """
      query = {
          "size": top_k,
          "query": {
              "knn": {
                  "embedding": {
                      "vector": query_vector,
                      "k": top_k
                  }
              }
          }
      }

      if filters:
          query["query"] = {
              "bool": {
                  "must": [query["query"]],
                  "filter": filters
              }
          }

      response = self.client.search(
          index=index_name,
          body=query
      )

      return self._parse_search_results(response)
  ```

- [ ] **å®ç°BM25æ£€ç´¢**
  ```python
  def bm25_search(
      self,
      index_name: str,
      query_text: str,
      top_k: int = 20
  ):
      """
      BM25å…³é”®è¯æ£€ç´¢

      Args:
          index_name: ç´¢å¼•åç§°
          query_text: æŸ¥è¯¢æ–‡æœ¬
          top_k: è¿”å›æ•°é‡

      Returns:
          List of search results
      """
      query = {
          "size": top_k,
          "query": {
              "multi_match": {
                  "query": query_text,
                  "fields": ["content^2", "content_with_context"],
                  "type": "best_fields"
              }
          }
      }

      response = self.client.search(
          index=index_name,
          body=query
      )

      return self._parse_search_results(response)
  ```

- [ ] **å®ç°Hybrid Searchï¼ˆRRFåˆå¹¶ï¼‰**
  ```python
  # app/services/query.py ä¸­æ·»åŠ 

  def hybrid_search(
      self,
      index_name: str,
      queries: List[str],
      top_k: int = 20
  ) -> List[dict]:
      """
      æ··åˆæ£€ç´¢ï¼šå‘é‡æ£€ç´¢ + BM25æ£€ç´¢

      Args:
          index_name: ç´¢å¼•åç§°
          queries: æŸ¥è¯¢åˆ—è¡¨ï¼ˆåŒ…å«åŸå§‹å’Œæ”¹å†™ï¼‰
          top_k: æœ€ç»ˆè¿”å›æ•°é‡

      Returns:
          List of search results (sorted by score)
      """
      all_results = []

      for query in queries:
          # å‘é‡æ£€ç´¢
          query_vector = self.bedrock.generate_embedding(query)
          vector_results = self.opensearch.vector_search(
              index_name, query_vector, top_k * 2
          )

          # BM25æ£€ç´¢
          bm25_results = self.opensearch.bm25_search(
              index_name, query, top_k * 2
          )

          all_results.append((vector_results, bm25_results))

      # ä½¿ç”¨RRFåˆå¹¶ç»“æœ
      merged_results = self._reciprocal_rank_fusion(
          all_results, k=60
      )

      # å»é‡å’Œæ’åº
      deduplicated = self._deduplicate_results(merged_results)

      return deduplicated[:top_k]

  def _reciprocal_rank_fusion(
      self,
      result_lists: List[tuple],
      k: int = 60
  ) -> List[dict]:
      """
      Reciprocal Rank Fusionç®—æ³•åˆå¹¶å¤šä¸ªæ£€ç´¢ç»“æœ

      Args:
          result_lists: å¤šç»„æ£€ç´¢ç»“æœ
          k: RRFå‚æ•°

      Returns:
          åˆå¹¶åçš„ç»“æœ
      """
      scores = {}

      for vector_results, bm25_results in result_lists:
          # å‘é‡æ£€ç´¢ç»“æœ
          for rank, result in enumerate(vector_results, 1):
              chunk_id = result['chunk_id']
              if chunk_id not in scores:
                  scores[chunk_id] = {'score': 0, 'result': result}
              scores[chunk_id]['score'] += 1 / (k + rank)

          # BM25ç»“æœ
          for rank, result in enumerate(bm25_results, 1):
              chunk_id = result['chunk_id']
              if chunk_id not in scores:
                  scores[chunk_id] = {'score': 0, 'result': result}
              scores[chunk_id]['score'] += 1 / (k + rank)

      # æ’åº
      sorted_results = sorted(
          scores.values(),
          key=lambda x: x['score'],
          reverse=True
      )

      return [r['result'] for r in sorted_results]

  def _deduplicate_results(self, results: List[dict]) -> List[dict]:
      """å»é‡"""
      seen = set()
      deduplicated = []

      for result in results:
          chunk_id = result['chunk_id']
          if chunk_id not in seen:
              seen.add(chunk_id)
              deduplicated.append(result)

      return deduplicated
  ```

- [ ] **æµ‹è¯•Hybrid Search**
  ```python
  def test_hybrid_search():
      service = QueryService(bedrock, opensearch)
      results = service.hybrid_search(
          "kb_test_index",
          ["ç™»å½•åŠŸèƒ½", "ç”¨æˆ·ç™»å½•", "è®¤è¯æ–¹å¼"],
          top_k=10
      )

      assert len(results) <= 10
      assert all('chunk_id' in r for r in results)
  ```

### 1.3 æ–‡æ¡£èšåˆ (Day 4)

- [ ] **å®ç°æŒ‰æ–‡æ¡£èšåˆchunks**
  ```python
  # app/services/query.py ä¸­æ·»åŠ 

  def aggregate_by_document(
      self,
      search_results: List[dict]
  ) -> dict:
      """
      å°†æ£€ç´¢åˆ°çš„chunksæŒ‰æ–‡æ¡£èšåˆ

      Args:
          search_results: æ£€ç´¢ç»“æœ

      Returns:
          {document_id: [chunk1, chunk2, ...]}
      """
      aggregated = {}

      for result in search_results:
          doc_id = result['document_id']
          if doc_id not in aggregated:
              aggregated[doc_id] = []
          aggregated[doc_id].append(result)

      return aggregated
  ```

---

## ç¬¬2å‘¨ï¼šMulti-Agentç³»ç»Ÿ

### 2.1 Sub-Agentå®ç° (Day 5-7)

- [ ] **å®ç°Sub-Agent**
  ```python
  # app/agents/sub_agent.py

  class SubAgent:
      def __init__(self, bedrock: BedrockService):
          self.bedrock = bedrock

      async def process_document(
          self,
          document_id: str,
          query: str,
          relevant_chunks: List[dict]
      ) -> dict:
          """
          é˜…è¯»å•ä¸ªæ–‡æ¡£å¹¶ç”Ÿæˆå›ç­”

          Args:
              document_id: æ–‡æ¡£ID
              query: ç”¨æˆ·é—®é¢˜
              relevant_chunks: ç›¸å…³chunks

          Returns:
              {
                  "document_id": str,
                  "document_name": str,
                  "answer": str,
                  "citations": [chunk_id, ...]
              }
          """
          # 1. è·å–æ–‡æ¡£å†…å®¹ï¼ˆä¼˜å…ˆæœ¬åœ°ç¼“å­˜ï¼‰
          markdown_path = await self._get_document_content(document_id)

          # 2. è¯»å–æ–‡æ¡£
          with open(markdown_path, 'r', encoding='utf-8') as f:
              markdown_content = f.read()

          # 3. åŠ è½½ç›¸å…³å›¾ç‰‡
          images = await self._load_relevant_images(relevant_chunks)

          # 4. æ„å»ºPrompt
          prompt = self._build_prompt(query, markdown_content, relevant_chunks)

          # 5. è°ƒç”¨Bedrockï¼ˆå¤šæ¨¡æ€ï¼‰
          response = await self.bedrock.generate_with_images(
              text=prompt,
              images=images
          )

          # 6. è§£æå“åº”
          result = self._parse_response(response)

          return {
              "document_id": document_id,
              "document_name": self._get_document_name(document_id),
              "answer": result['answer'],
              "citations": result['citations']
          }

      def _build_prompt(
          self,
          query: str,
          markdown_content: str,
          relevant_chunks: List[dict]
      ) -> str:
          """æ„å»ºPrompt"""
          # æ ¼å¼åŒ–ç›¸å…³chunks
          chunks_text = "\n\n".join([
              f"ã€ç‰‡æ®µ{i+1} - chunk_id: {c['chunk_id']}ã€‘\n{c['content']}"
              for i, c in enumerate(relevant_chunks)
          ])

          prompt = f"""
          ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„äº§å“æ–‡æ¡£åˆ†æåŠ©æ‰‹ã€‚è¯·ä»”ç»†é˜…è¯»ä»¥ä¸‹PRDæ–‡æ¡£ï¼Œå›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

          ç”¨æˆ·é—®é¢˜ï¼š
          {query}

          å®Œæ•´æ–‡æ¡£å†…å®¹ï¼š
          {markdown_content}

          é‡ç‚¹å…³æ³¨çš„ç‰‡æ®µï¼š
          {chunks_text}

          è¯·è¾“å‡ºJSONæ ¼å¼çš„å›ç­”ï¼š
          {{
              "answer": "é’ˆå¯¹ç”¨æˆ·é—®é¢˜çš„å›ç­”",
              "citations": ["chunk_id1", "chunk_id2", ...]
          }}

          è¦æ±‚ï¼š
          1. å›ç­”è¦å‡†ç¡®ã€å®Œæ•´
          2. citationsä¸­åˆ—å‡ºä½ å¼•ç”¨çš„chunk_id
          3. å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œæ˜ç¡®è¯´æ˜
          """

          return prompt

      def _parse_response(self, response: str) -> dict:
          """è§£æBedrockå“åº”"""
          try:
              import json
              result = json.loads(response)
              return result
          except:
              # å¦‚æœä¸æ˜¯JSONæ ¼å¼ï¼Œå°è¯•æå–
              return {
                  "answer": response,
                  "citations": []
              }

      async def _get_document_content(self, document_id: str) -> str:
          """è·å–æ–‡æ¡£å†…å®¹ï¼ˆæœ¬åœ°ç¼“å­˜ä¼˜å…ˆï¼‰"""
          # ä»æ•°æ®åº“æŸ¥è¯¢
          doc = self.db.query(Document).filter_by(id=document_id).first()

          if doc.local_markdown_path and os.path.exists(doc.local_markdown_path):
              return doc.local_markdown_path

          # ä»S3ä¸‹è½½
          local_path = f"/data/cache/documents/{document_id}/content.md"
          await self.s3.download_file(doc.s3_key_markdown, local_path)

          # æ›´æ–°æ•°æ®åº“
          doc.local_markdown_path = local_path
          self.db.commit()

          return local_path

      async def _load_relevant_images(self, relevant_chunks: List[dict]) -> List[str]:
          """åŠ è½½ç›¸å…³å›¾ç‰‡"""
          images = []

          for chunk in relevant_chunks:
              if chunk.get('chunk_type') == 'image':
                  # è·å–å›¾ç‰‡è·¯å¾„
                  chunk_obj = self.db.query(Chunk).filter_by(id=chunk['chunk_id']).first()

                  if chunk_obj.image_local_path and os.path.exists(chunk_obj.image_local_path):
                      images.append(chunk_obj.image_local_path)
                  else:
                      # ä»S3ä¸‹è½½
                      local_path = f"/data/cache/images/{chunk_obj.id}.png"
                      await self.s3.download_file(chunk_obj.image_s3_key, local_path)
                      images.append(local_path)

          return images
  ```

### 2.2 Main Agentå®ç° (Day 7-8)

- [ ] **å®ç°Main Agent**
  ```python
  # app/agents/main_agent.py

  from concurrent.futures import ThreadPoolExecutor, as_completed

  class MainAgent:
      def __init__(self, bedrock: BedrockService):
          self.bedrock = bedrock
          self.sub_agent = SubAgent(bedrock)

      async def process_query(
          self,
          query: str,
          kb_id: str
      ) -> dict:
          """
          å¤„ç†ç”¨æˆ·æŸ¥è¯¢ï¼ˆå®Œæ•´æµç¨‹ï¼‰

          Args:
              query: ç”¨æˆ·é—®é¢˜
              kb_id: çŸ¥è¯†åº“ID

          Returns:
              {
                  "query_id": str,
                  "answer": str,
                  "citations": [...],
                  "tokens": {...}
              }
          """
          # 1. Query Rewrite
          rewritten_queries = await self.query_service.rewrite_query(query)

          # 2. Hybrid Search
          index_name = f"kb_{kb_id}_index"
          search_results = self.query_service.hybrid_search(
              index_name, rewritten_queries, top_k=20
          )

          if not search_results:
              raise NoDocumentsFoundError("æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")

          # 3. æŒ‰æ–‡æ¡£èšåˆ
          aggregated = self.query_service.aggregate_by_document(search_results)

          # 4. å¹¶å‘æ‰§è¡ŒSub-Agent
          sub_agent_results = await self._run_sub_agents(
              query, aggregated
          )

          if not sub_agent_results:
              raise AgentExecutionError("æ‰€æœ‰æ–‡æ¡£å¤„ç†éƒ½å¤±è´¥äº†")

          # 5. Main Agentç»¼åˆ
          final_answer = await self._synthesize_answer(
              query, sub_agent_results
          )

          # 6. ä¿å­˜æŸ¥è¯¢å†å²
          query_record = await self._save_query_history(
              kb_id, query, rewritten_queries, final_answer
          )

          return {
              "query_id": query_record.id,
              "answer": final_answer['answer'],
              "citations": final_answer['citations'],
              "tokens": final_answer['tokens']
          }

      async def _run_sub_agents(
          self,
          query: str,
          aggregated: dict
      ) -> List[dict]:
          """å¹¶å‘è¿è¡ŒSub-Agent"""
          results = []
          failed_docs = []

          # é™åˆ¶å¹¶å‘æ•°ä¸º5
          with ThreadPoolExecutor(max_workers=5) as executor:
              futures = {
                  executor.submit(
                      self.sub_agent.process_document,
                      doc_id,
                      query,
                      chunks
                  ): doc_id
                  for doc_id, chunks in aggregated.items()
              }

              for future in as_completed(futures):
                  doc_id = futures[future]
                  try:
                      result = future.result(timeout=60)  # 60ç§’è¶…æ—¶
                      results.append(result)
                  except TimeoutError:
                      logger.error(f"Sub-agent timeout: {doc_id}")
                      failed_docs.append(doc_id)
                  except Exception as e:
                      logger.error(f"Sub-agent error: {doc_id}, {e}")
                      failed_docs.append(doc_id)

          if failed_docs:
              logger.warning(f"Sub-agents partial failure: {failed_docs}")

          return results

      async def _synthesize_answer(
          self,
          query: str,
          sub_agent_results: List[dict]
      ) -> dict:
          """ç»¼åˆSub-Agentçš„ç»“æœ"""
          # æ„å»ºPrompt
          sub_results_text = "\n\n".join([
              f"ã€æ–‡æ¡£ï¼š{r['document_name']}ã€‘\n{r['answer']}"
              for r in sub_agent_results
          ])

          prompt = f"""
          ä½ æ˜¯ä¸€ä¸ªäº§å“çŸ¥è¯†é—®ç­”åŠ©æ‰‹ã€‚ç°åœ¨æœ‰å¤šä¸ªæ–‡æ¡£çš„åˆ†æç»“æœï¼Œè¯·ç»¼åˆå›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

          ç”¨æˆ·é—®é¢˜ï¼š
          {query}

          å„æ–‡æ¡£çš„åˆ†æç»“æœï¼š
          {sub_results_text}

          è¯·ç»¼åˆä»¥ä¸Šä¿¡æ¯ï¼Œç”Ÿæˆä¸€ä¸ªå®Œæ•´ã€å‡†ç¡®ã€ç»“æ„åŒ–çš„å›ç­”ã€‚
          å¦‚æœä¸åŒæ–‡æ¡£æœ‰æ¼”è¿›å…³ç³»ï¼Œè¯·æŒ‰æ—¶é—´é¡ºåºç»„ç»‡ç­”æ¡ˆã€‚

          åœ¨ç›¸å…³æ®µè½åä½¿ç”¨[^1][^2]æ ‡æ³¨å¼•ç”¨ã€‚
          """

          # æµå¼ç”Ÿæˆ
          answer_chunks = []
          async for chunk in self.bedrock.generate_text_stream(prompt):
              answer_chunks.append(chunk)

          answer = "".join(answer_chunks)

          # æ”¶é›†æ‰€æœ‰citations
          all_citations = []
          for r in sub_agent_results:
              for chunk_id in r['citations']:
                  # ä»æ•°æ®åº“è·å–chunkè¯¦æƒ…
                  chunk = self.db.query(Chunk).filter_by(id=chunk_id).first()
                  citation = self._format_citation(chunk)
                  all_citations.append(citation)

          return {
              "answer": answer,
              "citations": all_citations,
              "tokens": {
                  "prompt_tokens": 0,  # TODO: è®¡ç®—
                  "completion_tokens": 0,  # TODO: è®¡ç®—
                  "total_tokens": 0
              }
          }

      def _format_citation(self, chunk: Chunk) -> dict:
          """æ ¼å¼åŒ–å¼•ç”¨"""
          doc = self.db.query(Document).filter_by(id=chunk.document_id).first()

          if chunk.chunk_type == 'text':
              return {
                  "chunk_id": chunk.id,
                  "chunk_type": "text",
                  "document_id": chunk.document_id,
                  "document_name": doc.filename,
                  "content": chunk.content,
                  "chunk_index": chunk.chunk_index
              }
          else:  # image
              return {
                  "chunk_id": chunk.id,
                  "chunk_type": "image",
                  "document_id": chunk.document_id,
                  "document_name": doc.filename,
                  "image_url": f"/api/v1/chunks/{chunk.id}/image",
                  "image_description": chunk.image_description,
                  "chunk_index": chunk.chunk_index
              }
  ```

---

## ç¬¬3å‘¨ï¼šæµå¼è¾“å‡ºå’ŒAPIå®ç°

### 3.1 æµå¼è¾“å‡ºï¼ˆSSEï¼‰ (Day 9-10)

- [ ] **å®ç°æµå¼æŸ¥è¯¢API**
  ```python
  # app/api/v1/query.py

  from fastapi import APIRouter
  from fastapi.responses import StreamingResponse
  from sse_starlette.sse import EventSourceResponse

  router = APIRouter()

  @router.post("/knowledge-bases/{kb_id}/query/stream")
  async def stream_query(
      kb_id: str,
      request: QueryRequest,
      db = Depends(get_db)
  ):
      """æµå¼é—®ç­”"""
      async def event_generator():
          try:
              # çŠ¶æ€: é‡å†™æŸ¥è¯¢
              yield {
                  "event": "status",
                  "data": json.dumps({
                      "status": "rewriting_query",
                      "message": "æ­£åœ¨ä¼˜åŒ–æŸ¥è¯¢..."
                  }, ensure_ascii=False)
              }

              # ... Query Rewrite

              # çŠ¶æ€: æ£€ç´¢
              yield {
                  "event": "status",
                  "data": json.dumps({
                      "status": "searching",
                      "message": "æ­£åœ¨æ£€ç´¢æ–‡æ¡£..."
                  }, ensure_ascii=False)
              }

              # ... Hybrid Search

              # æ£€ç´¢åˆ°çš„æ–‡æ¡£
              yield {
                  "event": "retrieved_documents",
                  "data": json.dumps({
                      "document_ids": [doc_id for doc_id in aggregated.keys()],
                      "document_names": [get_doc_name(doc_id) for doc_id in aggregated.keys()]
                  }, ensure_ascii=False)
              }

              # çŠ¶æ€: é˜…è¯»æ–‡æ¡£
              for idx, doc_id in enumerate(aggregated.keys(), 1):
                  yield {
                      "event": "status",
                      "data": json.dumps({
                          "status": "reading_documents",
                          "message": f"æ­£åœ¨é˜…è¯»æ–‡æ¡£ {idx}/{len(aggregated)}"
                      }, ensure_ascii=False)
                  }

              # ... Sub-Agentå¤„ç†

              # çŠ¶æ€: ç”Ÿæˆç­”æ¡ˆ
              yield {
                  "event": "status",
                  "data": json.dumps({
                      "status": "generating",
                      "message": "æ­£åœ¨ç”Ÿæˆç­”æ¡ˆ..."
                  }, ensure_ascii=False)
              }

              # æµå¼ç”Ÿæˆç­”æ¡ˆ
              async for chunk in main_agent.generate_answer_stream(query, sub_results):
                  yield {
                      "event": "chunk",
                      "data": json.dumps({
                          "content": chunk
                      }, ensure_ascii=False)
                  }

              # å¼•ç”¨
              for citation in final_answer['citations']:
                  yield {
                      "event": "citation",
                      "data": json.dumps(citation, ensure_ascii=False)
                  }

              # Tokenç»Ÿè®¡
              yield {
                  "event": "tokens",
                  "data": json.dumps(final_answer['tokens'], ensure_ascii=False)
              }

              # å®Œæˆ
              yield {
                  "event": "done",
                  "data": json.dumps({
                      "query_id": query_record.id
                  }, ensure_ascii=False)
              }

          except Exception as e:
              logger.error(f"Query failed: {str(e)}", exc_info=True)
              yield {
                  "event": "error",
                  "data": json.dumps({
                      "code": "8001",
                      "message": str(e)
                  }, ensure_ascii=False)
              }

      return EventSourceResponse(event_generator())
  ```

### 3.2 æŸ¥è¯¢å†å²API (Day 10)

- [ ] **å®ç°æŸ¥è¯¢å†å²API**
  ```python
  @router.get("/knowledge-bases/{kb_id}/query-history")
  async def list_query_history(
      kb_id: str,
      page: int = 1,
      page_size: int = 20,
      db = Depends(get_db)
  ):
      """è·å–æŸ¥è¯¢å†å²åˆ—è¡¨"""
      pass

  @router.get("/query-history/{query_id}")
  async def get_query_detail(query_id: str, db = Depends(get_db)):
      """è·å–æŸ¥è¯¢è¯¦æƒ…"""
      pass

  @router.delete("/query-history/{query_id}")
  async def delete_query(query_id: str, db = Depends(get_db)):
      """åˆ é™¤æŸ¥è¯¢è®°å½•"""
      pass
  ```

### 3.3 å·¥å…·ç±»API (Day 10)

- [ ] **å®ç°å·¥å…·API**
  ```python
  # app/api/v1/utilities.py

  @router.get("/chunks/{chunk_id}/image")
  async def get_chunk_image(chunk_id: str, db = Depends(get_db)):
      """è·å–å›¾ç‰‡chunkçš„å›¾ç‰‡"""
      chunk = db.query(Chunk).filter_by(id=chunk_id).first()

      if not chunk or chunk.chunk_type != 'image':
          raise HTTPException(404, "å›¾ç‰‡ä¸å­˜åœ¨")

      # è·å–å›¾ç‰‡è·¯å¾„
      if chunk.image_local_path and os.path.exists(chunk.image_local_path):
          image_path = chunk.image_local_path
      else:
          # ä»S3ä¸‹è½½
          image_path = f"/tmp/{chunk.id}.png"
          await s3_service.download_file(chunk.image_s3_key, image_path)

      # è¿”å›å›¾ç‰‡
      return FileResponse(
          image_path,
          media_type="image/png",
          headers={"Cache-Control": "public, max-age=86400"}
      )

  @router.get("/stats")
  async def get_stats(kb_id: Optional[str] = None, db = Depends(get_db)):
      """è·å–ç»Ÿè®¡ä¿¡æ¯"""
      pass
  ```

---

## éªŒæ”¶æ ‡å‡†

### å¿…é¡»å®Œæˆ
- [ ] Query RewriteåŠŸèƒ½æ­£å¸¸
- [ ] Hybrid Searchå·¥ä½œæ­£å¸¸
- [ ] Sub-Agentå¯ä»¥æ­£ç¡®å¤„ç†æ–‡æ¡£
- [ ] Main Agentå¯ä»¥ç»¼åˆç”Ÿæˆç­”æ¡ˆ
- [ ] æµå¼è¾“å‡ºå·¥ä½œæ­£å¸¸
- [ ] å¼•ç”¨æå–æ­£ç¡®ï¼ˆæ–‡æœ¬+å›¾ç‰‡ï¼‰
- [ ] æŸ¥è¯¢å†å²ä¿å­˜æ­£å¸¸
- [ ] å›¾ç‰‡å¯ä»¥æ­£ç¡®å±•ç¤º
- [ ] ç«¯åˆ°ç«¯æµ‹è¯•é€šè¿‡

### å¯é€‰
- [ ] æŸ¥è¯¢ç¼“å­˜
- [ ] ç­”æ¡ˆè´¨é‡è¯„åˆ†
- [ ] ç”¨æˆ·åé¦ˆåŠŸèƒ½

---

## æ£€æŸ¥æ¸…å•

åœ¨è¿›å…¥Phase 4ä¹‹å‰ï¼Œç¡®è®¤ï¼š

- [ ] å¯ä»¥æˆåŠŸæé—®å¹¶å¾—åˆ°ç­”æ¡ˆ
- [ ] ç­”æ¡ˆæµå¼è¾“å‡ºæ­£å¸¸
- [ ] å¼•ç”¨ä¿¡æ¯æ­£ç¡®
- [ ] å›¾ç‰‡å¯ä»¥æ­£ç¡®å±•ç¤º
- [ ] æŸ¥è¯¢å†å²å¯ä»¥æŸ¥çœ‹
- [ ] Tokenç»Ÿè®¡å‡†ç¡®
- [ ] é”™è¯¯å¤„ç†å®Œå–„
- [ ] æ€§èƒ½å¯æ¥å—ï¼ˆ< 30ç§’ï¼‰
- [ ] å®Œæ•´æµç¨‹æµ‹è¯•é€šè¿‡

---

## ä¸‹ä¸€æ­¥

å®ŒæˆPhase 3åï¼Œè¿›å…¥ [Phase 4: å‰ç«¯å¼€å‘](./todo-phase4-frontend.md)
